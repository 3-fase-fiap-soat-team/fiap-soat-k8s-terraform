#!/bin/bash

# Script de deploy para FIAP SOAT - AWS Academy
# Autor: rs94458
# Uso: ./scripts/deploy.sh
# Vers√£o 2.0 - Com limpeza robusta e deploy automatizado

set -e

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configura√ß√µes
CLUSTER_NAME="fiap-soat-cluster"
AWS_REGION="us-east-1"
ACCOUNT_ID="280273007505"
ECR_REPOSITORY="fiap-soat-nestjs-app"
IMAGE_TAG="latest"
APP_NAMESPACE="fiap-soat-app"

# Detectar diret√≥rio base do projeto
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
TERRAFORM_DIR="$PROJECT_DIR/environments/dev"
MANIFESTS_DIR="$PROJECT_DIR/manifests"

MAX_RETRIES=3
CREDENTIAL_CHECK_INTERVAL=300 # 5 minutos

# Fun√ß√£o de log
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[WARNING] $1${NC}"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}"
    exit 1
}

info() {
    echo -e "${BLUE}[INFO] $1${NC}"
}

success() {
    echo -e "${GREEN}[SUCCESS] ‚úÖ $1${NC}"
}

# Verificar depend√™ncias
check_dependencies() {
    log "Verificando depend√™ncias..."
    
    command -v terraform >/dev/null 2>&1 || error "Terraform n√£o encontrado. Instale: https://terraform.io/"
    command -v kubectl >/dev/null 2>&1 || error "kubectl n√£o encontrado. Instale: https://kubernetes.io/docs/tasks/tools/"
    command -v aws >/dev/null 2>&1 || error "AWS CLI n√£o encontrado. Instale: https://aws.amazon.com/cli/"
    
    success "Depend√™ncias verificadas"
}

# Verificar e renovar credenciais AWS
check_aws_credentials() {
    log "Verificando credenciais AWS..."
    
    local retry_count=0
    while [ $retry_count -lt $MAX_RETRIES ]; do
        if aws sts get-caller-identity >/dev/null 2>&1; then
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            USER_ARN=$(aws sts get-caller-identity --query Arn --output text)
            
            # Verificar se s√£o credenciais tempor√°rias (AWS Academy)
            if echo "$USER_ARN" | grep -q "assumed-role"; then
                info "Credenciais tempor√°rias detectadas (AWS Academy)"
                
                # Verificar tempo restante (aproximado)
                local session_token=$(aws configure get aws_session_token)
                if [ -n "$session_token" ]; then
                    warn "‚è∞ Lembre-se: credenciais AWS Academy expiram em ~3h"
                fi
            fi
            
            success "Conectado √† conta AWS: $ACCOUNT_ID"
            return 0
        else
            retry_count=$((retry_count + 1))
            warn "Falha na verifica√ß√£o de credenciais (tentativa $retry_count/$MAX_RETRIES)"
            
            if [ $retry_count -lt $MAX_RETRIES ]; then
                info "üí° Configure novas credenciais com: ./scripts/aws-config.sh"
                read -p "Pressione Enter ap√≥s configurar as credenciais ou Ctrl+C para cancelar..."
            fi
        fi
    done
    
    error "Credenciais AWS inv√°lidas ap√≥s $MAX_RETRIES tentativas"
}

# Verificar se credenciais ainda est√£o v√°lidas
verify_credentials() {
    if ! aws sts get-caller-identity >/dev/null 2>&1; then
        warn "üîÑ Credenciais expiraram durante a opera√ß√£o"
        info "üí° Renove as credenciais com: ./scripts/aws-config.sh"
        read -p "Pressione Enter ap√≥s renovar as credenciais..."
        check_aws_credentials
    fi
}

# Limpar state √≥rf√£o e inconsistente
clean_terraform_state() {
    log "üßπ Limpando estado do Terraform..."
    
    cd "$TERRAFORM_DIR"
    
    # Verificar se h√° state file
    if [ ! -f "terraform.tfstate" ]; then
        info "Nenhum state file encontrado - deploy limpo"
        check_orphaned_resources_before_deploy
    else
        # Verificar consist√™ncia do state com AWS
        check_state_consistency
    fi
    
    # Limpar planos e backups √≥rf√£os
    rm -f tfplan terraform.tfstate.backup.drift-* 2>/dev/null || true
    
    success "State limpo"
    cd - >/dev/null
}

# Verificar recursos √≥rf√£os antes do deploy
check_orphaned_resources_before_deploy() {
    log "üîç Verificando recursos √≥rf√£os antes do deploy..."
    
    # Verificar VPCs √≥rf√£s do projeto
    local vpc_ids=$(aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].VpcId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$vpc_ids" ] && [ "$vpc_ids" != "None" ]; then
        warn "üö® Detectadas VPCs √≥rf√£s do projeto: $vpc_ids"
        echo
        echo "Op√ß√µes dispon√≠veis:"
        echo "1) üóëÔ∏è  Remover VPCs √≥rf√£s (recomendado para deploy limpo)"
        echo "2) üîÑ Tentar reutilizar VPCs existentes (arriscado)"
        echo "3) ‚è≠Ô∏è  Continuar e deixar o Terraform decidir"
        echo
        read -p "Escolha (1-3): " vpc_choice
        
        case $vpc_choice in
            1)
                log "Removendo VPCs √≥rf√£s antes do deploy..."
                for vpc_id in $vpc_ids; do
                    cleanup_single_vpc "$vpc_id"
                done
                ;;
            2)
                warn "‚ö†Ô∏è  ARRISCADO: Terraform pode dar conflito ou erro"
                warn "üí° Monitore o plano do Terraform cuidadosamente"
                ;;
            3)
                info "Continuando com deploy - Terraform tentar√° resolver conflitos"
                warn "‚ö†Ô∏è  Se houver erro, execute limpeza de √≥rf√£os (op√ß√£o 8 do menu)"
                ;;
            *)
                error "Op√ß√£o inv√°lida"
                ;;
        esac
    fi
    
    # Verificar outros recursos √≥rf√£os
    local clusters=$(aws eks list-clusters --query "clusters[?contains(@, \`$CLUSTER_NAME\`)]" --output text 2>/dev/null || true)
    if [ -n "$clusters" ] && [ "$clusters" != "None" ]; then
        warn "üö® Cluster EKS √≥rf√£o detectado: $clusters"
        warn "‚ö†Ô∏è  Terraform pode dar erro ao tentar criar cluster com mesmo nome"
        echo
        read -p "Remover cluster √≥rf√£o antes do deploy? (s/N): " remove_cluster
        
        if [[ "$remove_cluster" =~ ^[Ss]$ ]]; then
            aws eks delete-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" || true
            log "Aguardando remo√ß√£o do cluster..."
            sleep 60
        fi
    fi
}

# Verificar consist√™ncia do state com AWS  
check_state_consistency() {
    log "üîç Verificando consist√™ncia do state com AWS..."
    
    # Verificar se recursos no state ainda existem no AWS
    local resources=$(terraform state list 2>/dev/null || true)
    
    if [ -n "$resources" ]; then
        echo "$resources" | while read resource; do
            case $resource in
                *aws_eks_cluster*)
                    local cluster_name_state=$(terraform state show "$resource" 2>/dev/null | grep "name.*=" | awk '{print $3}' | tr -d '"' || true)
                    if [ -n "$cluster_name_state" ]; then
                        if ! aws eks describe-cluster --name "$cluster_name_state" --region "$AWS_REGION" >/dev/null 2>&1; then
                            warn "Cluster '$cluster_name_state' no state mas n√£o existe no AWS"
                            warn "Removendo do state: $resource"
                            terraform state rm "$resource" || true
                        fi
                    fi
                    ;;
                *aws_vpc*)
                    local vpc_id_state=$(terraform state show "$resource" 2>/dev/null | grep "id.*=" | awk '{print $3}' | tr -d '"' || true)
                    if [ -n "$vpc_id_state" ]; then
                        if ! aws ec2 describe-vpcs --vpc-ids "$vpc_id_state" >/dev/null 2>&1; then
                            warn "VPC '$vpc_id_state' no state mas n√£o existe no AWS"
                            warn "Removendo do state: $resource"
                            terraform state rm "$resource" || true
                        fi
                    fi
                    ;;
            esac
        done
    fi
}

# Deploy robusto da infraestrutura
deploy_infrastructure() {
    log "üöÄ Iniciando deploy da infraestrutura..."
    
    cd "$TERRAFORM_DIR"
    
    # Verificar credenciais antes de come√ßar
    verify_credentials
    
    # Limpar state inconsistente
    clean_terraform_state
    
    # Inicializar Terraform
    log "Inicializando Terraform..."
    terraform init
    
    # Validar configura√ß√£o
    log "Validando configura√ß√£o..."
    terraform validate
    
    # Mostrar plano e detectar conflitos
    log "Criando plano de execu√ß√£o..."
    if ! terraform plan -out=tfplan; then
        error "‚ùå Falha no plano do Terraform!"
        warn "üí° Poss√≠veis causas:"
        warn "   ‚Ä¢ Recursos √≥rf√£os conflitando"
        warn "   ‚Ä¢ CIDR blocks duplicados" 
        warn "   ‚Ä¢ Tags conflitantes"
        warn "   ‚Ä¢ Nomes de recursos j√° existentes"
        echo
        warn "üîß Solu√ß√µes:"
        warn "   1) Execute: ./scripts/deploy.sh (op√ß√£o 8 - Limpar √≥rf√£os)"
        warn "   2) Execute: ./scripts/deploy.sh (op√ß√£o 9 - Limpar state)"
        warn "   3) Verifique manualmente no console AWS"
        exit 1
    fi
    
    # Verificar se o plano indica recursos √≥rf√£os sendo importados/conflitados
    log "Analisando plano para detectar conflitos reais..."
    if terraform show tfplan | grep -q "will be imported\|already exists" | grep -v "resolve_conflicts"; then
        warn "‚ö†Ô∏è  Detectados poss√≠veis conflitos no plano:"
        terraform show tfplan | grep -A 2 -B 2 "will be imported\|already exists" | grep -v "resolve_conflicts" || true
        echo
        read -p "Continuar mesmo assim? (s/N): " continue_with_conflicts
        
        if [[ ! "$continue_with_conflicts" =~ ^[Ss]$ ]]; then
            error "Deploy cancelado devido a conflitos detectados"
        fi
    else
        info "Plano validado - nenhum conflito real detectado"
    fi
    
    # Confirmar aplica√ß√£o
    warn "‚ö†Ô∏è  ATEN√á√ÉO: Este deploy criar√° recursos que CUSTAM DINHEIRO na AWS!"
    warn "üí∞ EKS Control Plane: ~$73/m√™s"
    warn "üí∞ Node Group (t3.small): ~$15/m√™s"
    warn "üí∞ Total estimado: ~$88/m√™s (pode estourar budget AWS Academy!)"
    echo
    read -p "Deseja continuar? (digite 'sim' para confirmar): " confirm
    
    if [ "$confirm" != "sim" ]; then
        error "Deploy cancelado pelo usu√°rio"
    fi
    
    # Aplicar mudan√ßas com retry em caso de expira√ß√£o de credenciais
    local apply_success=false
    local retry_count=0
    
    while [ $retry_count -lt $MAX_RETRIES ] && [ "$apply_success" = false ]; do
        log "Aplicando mudan√ßas (tentativa $((retry_count + 1))/$MAX_RETRIES)..."
        
        if terraform apply tfplan; then
            apply_success=true
            success "Infraestrutura criada com sucesso!"
        else
            retry_count=$((retry_count + 1))
            
            if [ $retry_count -lt $MAX_RETRIES ]; then
                warn "Falha no apply. Verificando se foi problema de credenciais..."
                verify_credentials
                
                # Recriar plano com credenciais renovadas
                log "Recriando plano com credenciais renovadas..."
                terraform plan -out=tfplan
            fi
        fi
    done
    
    if [ "$apply_success" = false ]; then
        error "Falha no deploy ap√≥s $MAX_RETRIES tentativas"
    fi
    
    # Aguardar cluster ficar ativo
    wait_for_cluster_ready
    
    cd - >/dev/null
}

# Aguardar cluster ficar pronto
wait_for_cluster_ready() {
    log "‚è≥ Aguardando cluster ficar ACTIVE..."
    
    local max_wait=900  # 15 minutos
    local wait_time=0
    local check_interval=30
    
    while [ $wait_time -lt $max_wait ]; do
        verify_credentials  # Verificar credenciais a cada check
        
        local status=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$status" = "ACTIVE" ]; then
            success "Cluster est√° ACTIVE!"
            return 0
        elif [ "$status" = "FAILED" ] || [ "$status" = "NOT_FOUND" ]; then
            error "Cluster falhou ou n√£o foi encontrado. Status: $status"
        else
            info "Status do cluster: $status (aguardando...)"
            sleep $check_interval
            wait_time=$((wait_time + check_interval))
        fi
    done
    
    error "Timeout aguardando cluster ficar ativo (${max_wait}s)"
}

# Remover aplica√ß√£o Kubernetes
cleanup_application() {
    log "üßπ Removendo aplica√ß√£o do Kubernetes..."
    
    if kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
        # Remover todos os recursos do namespace
        kubectl delete all --all -n $APP_NAMESPACE --ignore-not-found=true || true
        
        # Remover secrets espec√≠ficos
        kubectl delete secret ecr-secret -n $APP_NAMESPACE --ignore-not-found=true || true
        
        # Aguardar remo√ß√£o completa dos recursos
        log "Aguardando remo√ß√£o dos recursos..."
        sleep 10
        
        # For√ßar remo√ß√£o do namespace se ainda existir
        kubectl delete namespace $APP_NAMESPACE --ignore-not-found=true || true
        
        success "Aplica√ß√£o removida completamente"
    else
        info "Namespace '$APP_NAMESPACE' n√£o encontrado"
    fi
}

# Verificar recursos AWS restantes
check_aws_resources() {
    log "üìä Verificando recursos AWS restantes..."
    
    verify_credentials
    
    echo
    echo -e "${BLUE}=== üèóÔ∏è  EKS CLUSTERS ===${NC}"
    aws eks list-clusters --query "clusters[?contains(@, \`$CLUSTER_NAME\`)]" --output table 2>/dev/null || echo "Nenhum cluster encontrado"
    
    echo
    echo -e "${BLUE}=== üñ•Ô∏è  EC2 INSTANCES ===${NC}"
    aws ec2 describe-instances \
        --filters "Name=tag:Project,Values=fiap-soat*" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
        --query 'Reservations[].Instances[].{ID:InstanceId,State:State.Name,Type:InstanceType,Name:Tags[?Key==`Name`]|[0].Value}' \
        --output table 2>/dev/null || echo "Nenhuma inst√¢ncia encontrada"
    
    echo
    echo -e "${BLUE}=== ‚öñÔ∏è  LOAD BALANCERS ===${NC}"
    aws elbv2 describe-load-balancers \
        --query "LoadBalancers[?contains(LoadBalancerName, \`fiap-soat\`)].{Name:LoadBalancerName,State:State.Code,Type:Type}" \
        --output table 2>/dev/null || echo "Nenhum load balancer encontrado"
    
    echo
    echo -e "${BLUE}=== üåê VPCs ===${NC}"
    aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].{ID:VpcId,CIDR:CidrBlock,State:State,Name:Tags[?Key==`Name`]|[0].Value}' \
        --output table 2>/dev/null || echo "Nenhuma VPC encontrada"
    
    echo
    echo -e "${BLUE}=== üíæ VOLUMES EBS ===${NC}"
    aws ec2 describe-volumes \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Volumes[].{ID:VolumeId,Size:Size,State:State,Type:VolumeType}' \
        --output table 2>/dev/null || echo "Nenhum volume encontrado"
}

# Limpeza completa de recursos
cleanup_resources() {
    log "üßπ Iniciando limpeza completa de recursos..."
    
    # Mostrar recursos antes da destrui√ß√£o
    info "Verificando recursos que ser√£o destru√≠dos..."
    check_aws_resources
    
    echo
    # Confirmar limpeza
    warn "‚ö†Ô∏è  ATEN√á√ÉO: Isso ir√° DESTRUIR TODOS os recursos AWS criados!"
    warn "üíÄ Cluster EKS, Node Groups, VPC, Load Balancers, etc."
    warn "üí∞ Isso ir√° PARAR TODOS OS CUSTOS AWS!"
    warn "üî• Esta a√ß√£o √© IRREVERS√çVEL!"
    echo
    read -p "Tem certeza? Digite 'DESTRUIR' para confirmar: " confirm
    
    if [ "$confirm" != "DESTRUIR" ]; then
        info "Limpeza cancelada pelo usu√°rio"
        warn "üí∞ Lembre-se: recursos AWS continuam gerando custos!"
        return 0
    fi
    
    # Remover aplica√ß√£o primeiro
    cleanup_application
    
    cd "$TERRAFORM_DIR"
    verify_credentials
    
    # Mostrar plano de destroy
    log "Criando plano de destrui√ß√£o..."
    terraform plan -destroy -out=tfplan-destroy
    
    # Tentar destroy normal primeiro
    log "Executando destrui√ß√£o..."
    if terraform apply tfplan-destroy; then
        success "Destroy executado com sucesso!"
    else
        warn "Destroy falhou. Tentando limpeza for√ßada..."
        force_cleanup
    fi
    
    # Limpar contexto kubectl
    cleanup_kubectl_context
    
    # Limpar state e arquivos tempor√°rios
    cleanup_local_files
    
    # Verificar se restaram recursos √≥rf√£os
    log "Verificando recursos √≥rf√£os..."
    check_aws_resources
    
    success "üéâ Limpeza completa finalizada!"
    success "üí∞ Recursos AWS removidos - custos parados!"
    cd - >/dev/null
}

# Limpar contexto kubectl
cleanup_kubectl_context() {
    log "üßπ Limpando contexto kubectl..."
    
    if command -v kubectl >/dev/null 2>&1; then
        local current_context=$(kubectl config current-context 2>/dev/null || echo "")
        if [[ "$current_context" == *"fiap-soat"* ]]; then
            kubectl config delete-context "$current_context" 2>/dev/null || true
            kubectl config delete-cluster "$CLUSTER_NAME" 2>/dev/null || true
            success "Contexto kubectl removido"
        fi
    fi
}

# Limpar arquivos locais
cleanup_local_files() {
    log "üßπ Limpando arquivos tempor√°rios..."
    
    # Remover arquivos terraform
    rm -f terraform.tfstate* tfplan* .terraform.lock.hcl 2>/dev/null || true
    rm -rf .terraform/ 2>/dev/null || true
    
    # Remover backups antigos
    find . -name "terraform.tfstate.backup*" -delete 2>/dev/null || true
    find . -name "tfplan*" -delete 2>/dev/null || true
    
    success "Arquivos tempor√°rios limpos"
}

# Limpeza for√ßada para casos extremos
force_cleanup() {
    warn "‚ö†Ô∏è  Executando limpeza for√ßada de recursos √≥rf√£os..."
    
    # Confirmar limpeza for√ßada
    echo
    warn "üî• LIMPEZA FOR√áADA: Tentativa de remover recursos √≥rf√£os individualmente"
    warn "‚ö†Ô∏è  Isso pode deixar alguns recursos √≥rf√£os que custam dinheiro!"
    read -p "Continuar com limpeza for√ßada? (s/N): " force_confirm
    
    if [[ ! "$force_confirm" =~ ^[Ss]$ ]]; then
        info "Limpeza for√ßada cancelada"
        return 0
    fi
    
    # Listar recursos que ainda existem
    local resources=$(terraform state list 2>/dev/null || true)
    
    if [ -n "$resources" ]; then
        log "Removendo recursos individualmente..."
        
        # Remover node groups primeiro (depend√™ncias cr√≠ticas)
        echo "$resources" | grep "aws_eks_node_group" | while read resource; do
            log "Removendo node group: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 10
        done
        
        # Remover addons EKS
        echo "$resources" | grep "aws_eks_addon" | while read resource; do
            log "Removendo addon: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 5
        done
        
        # Aguardar node groups serem removidos
        log "Aguardando remo√ß√£o de node groups..."
        sleep 30
        
        # Remover cluster
        echo "$resources" | grep "aws_eks_cluster" | while read resource; do
            log "Removendo cluster: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 10
        done
        
        # Aguardar cluster ser removido
        log "Aguardando remo√ß√£o do cluster..."
        sleep 60
        
        # Tentar destroy completo final
        log "Tentando destroy completo final..."
        terraform destroy -auto-approve || warn "Alguns recursos podem n√£o ter sido removidos"
    fi
    
    # Limpeza manual de recursos √≥rf√£os
    cleanup_orphaned_resources
    
    # Verificar recursos restantes
    log "Verificando recursos restantes..."
    check_aws_resources
    
    warn "üö® IMPORTANTE: Verifique manualmente no console AWS se h√° recursos √≥rf√£os:"
    warn "   ‚Ä¢ EC2 ‚Üí Inst√¢ncias"
    warn "   ‚Ä¢ EKS ‚Üí Clusters"  
    warn "   ‚Ä¢ VPC ‚Üí Suas VPCs"
    warn "   ‚Ä¢ EC2 ‚Üí Load Balancers"
    warn "üí∞ Recursos √≥rf√£os podem continuar gerando custos!"
}

# Limpeza de recursos √≥rf√£os via AWS CLI
cleanup_orphaned_resources() {
    log "üîç Verificando recursos √≥rf√£os via AWS CLI..."
    
    verify_credentials
    
    # Verificar e remover node groups √≥rf√£os primeiro
    log "Verificando node groups..."
    local node_groups=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'nodegroups[]' --output text 2>/dev/null || true)
    
    if [ -n "$node_groups" ] && [ "$node_groups" != "None" ]; then
        warn "üö® Node groups √≥rf√£os detectados: $node_groups"
        for ng in $node_groups; do
            log "Removendo node group: $ng"
            aws eks delete-nodegroup \
                --cluster-name "$CLUSTER_NAME" \
                --nodegroup-name "$ng" \
                --region "$AWS_REGION" 2>/dev/null || true
        done
        
        # Aguardar remo√ß√£o dos node groups
        log "Aguardando remo√ß√£o dos node groups..."
        sleep 60
    fi
    
    # Verificar e remover addons √≥rf√£os
    log "Verificando addons EKS..."
    local addons=$(aws eks list-addons --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'addons[]' --output text 2>/dev/null || true)
    
    if [ -n "$addons" ] && [ "$addons" != "None" ]; then
        warn "üö® Addons √≥rf√£os detectados: $addons"
        for addon in $addons; do
            log "Removendo addon: $addon"
            aws eks delete-addon \
                --cluster-name "$CLUSTER_NAME" \
                --addon-name "$addon" \
                --region "$AWS_REGION" 2>/dev/null || true
        done
    fi
    
    # Verificar e remover cluster √≥rf√£o
    log "Verificando cluster EKS..."
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
        warn "üö® Cluster √≥rf√£o detectado: $CLUSTER_NAME"
        log "Removendo cluster..."
        aws eks delete-cluster \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" 2>/dev/null || true
        
        # Aguardar remo√ß√£o do cluster
        log "Aguardando remo√ß√£o do cluster..."
        sleep 90
    fi
    
    # Verificar inst√¢ncias EC2 √≥rf√£s do projeto
    log "Verificando inst√¢ncias EC2..."
    local instances=$(aws ec2 describe-instances \
        --filters "Name=tag:Project,Values=fiap-soat*" "Name=instance-state-name,Values=running,pending" \
        --query 'Reservations[].Instances[].InstanceId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$instances" ] && [ "$instances" != "None" ]; then
        warn "üö® Inst√¢ncias EC2 √≥rf√£s detectadas: $instances"
        warn "‚ö†Ô∏è  Considere remover manualmente no console AWS"
    fi
    
    # Verificar e limpar VPCs √≥rf√£s
    cleanup_orphaned_vpcs
    
    success "‚úÖ Verifica√ß√£o de recursos √≥rf√£os conclu√≠da"
}

# Limpeza espec√≠fica de VPCs √≥rf√£s
cleanup_orphaned_vpcs() {
    log "üîç Verificando VPCs √≥rf√£s do projeto..."
    
    # Buscar VPCs do projeto
    local vpc_ids=$(aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].VpcId' \
        --output text 2>/dev/null || true)
    
    if [ -z "$vpc_ids" ] || [ "$vpc_ids" = "None" ]; then
        info "Nenhuma VPC √≥rf√£ do projeto encontrada"
        return 0
    fi
    
    warn "üö® VPCs √≥rf√£s detectadas: $vpc_ids"
    echo
    read -p "Deseja remover as VPCs √≥rf√£s? (s/N): " vpc_confirm
    
    if [[ ! "$vpc_confirm" =~ ^[Ss]$ ]]; then
        info "Limpeza de VPCs cancelada"
        return 0
    fi
    
    for vpc_id in $vpc_ids; do
        log "üßπ Limpando VPC: $vpc_id"
        cleanup_single_vpc "$vpc_id"
    done
}

# Limpar uma VPC espec√≠fica e suas depend√™ncias
cleanup_single_vpc() {
    local vpc_id="$1"
    
    if [ -z "$vpc_id" ]; then
        error "VPC ID n√£o fornecido"
    fi
    
    log "Removendo depend√™ncias da VPC $vpc_id..."
    
    # 1. Remover inst√¢ncias EC2 (se houver)
    log "Verificando inst√¢ncias EC2 na VPC..."
    local instances=$(aws ec2 describe-instances \
        --filters "Name=vpc-id,Values=$vpc_id" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
        --query 'Reservations[].Instances[].InstanceId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$instances" ] && [ "$instances" != "None" ]; then
        warn "Terminando inst√¢ncias: $instances"
        aws ec2 terminate-instances --instance-ids $instances 2>/dev/null || true
        
        # Aguardar termina√ß√£o
        log "Aguardando termina√ß√£o das inst√¢ncias..."
        aws ec2 wait instance-terminated --instance-ids $instances 2>/dev/null || true
    fi
    
    # 2. Remover NAT Gateways
    log "Removendo NAT Gateways..."
    local nat_gateways=$(aws ec2 describe-nat-gateways \
        --filter "Name=vpc-id,Values=$vpc_id" \
        --query 'NatGateways[?State==`available`].NatGatewayId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$nat_gateways" ] && [ "$nat_gateways" != "None" ]; then
        for nat_id in $nat_gateways; do
            log "Removendo NAT Gateway: $nat_id"
            aws ec2 delete-nat-gateway --nat-gateway-id "$nat_id" 2>/dev/null || true
        done
        
        # Aguardar NAT Gateways serem removidos
        log "Aguardando remo√ß√£o dos NAT Gateways..."
        sleep 30
    fi
    
    # 3. Remover Internet Gateway
    log "Removendo Internet Gateway..."
    local igw_id=$(aws ec2 describe-internet-gateways \
        --filters "Name=attachment.vpc-id,Values=$vpc_id" \
        --query 'InternetGateways[].InternetGatewayId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$igw_id" ] && [ "$igw_id" != "None" ]; then
        log "Detaching e removendo Internet Gateway: $igw_id"
        aws ec2 detach-internet-gateway --internet-gateway-id "$igw_id" --vpc-id "$vpc_id" 2>/dev/null || true
        sleep 5
        aws ec2 delete-internet-gateway --internet-gateway-id "$igw_id" 2>/dev/null || true
    fi
    
    # 4. Remover Route Tables (n√£o default e n√£o associadas)
    log "Removendo Route Tables..."
    local route_tables=$(aws ec2 describe-route-tables \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$route_tables" ] && [ "$route_tables" != "None" ]; then
        for rt_id in $route_tables; do
            log "Limpando rotas da Route Table: $rt_id"
            
            # Primeiro, remover rotas customizadas (n√£o locais)
            local routes=$(aws ec2 describe-route-tables \
                --route-table-ids "$rt_id" \
                --query 'RouteTables[0].Routes[?GatewayId!=`local`].DestinationCidrBlock' \
                --output text 2>/dev/null || true)
                
            if [ -n "$routes" ] && [ "$routes" != "None" ]; then
                for cidr in $routes; do
                    log "Removendo rota $cidr da Route Table $rt_id"
                    aws ec2 delete-route --route-table-id "$rt_id" --destination-cidr-block "$cidr" 2>/dev/null || true
                done
            fi
            
            # Aguardar um pouco
            sleep 2
            
            log "Removendo Route Table: $rt_id"
            aws ec2 delete-route-table --route-table-id "$rt_id" 2>/dev/null || true
        done
    fi
    
    # 5. Remover Security Groups (n√£o default)
    log "Removendo Security Groups..."
    local security_groups=$(aws ec2 describe-security-groups \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$security_groups" ] && [ "$security_groups" != "None" ]; then
        for sg_id in $security_groups; do
            log "Removendo Security Group: $sg_id"
            aws ec2 delete-security-group --group-id "$sg_id" 2>/dev/null || true
        done
    fi
    
    # 6. Remover Subnets
    log "Removendo Subnets..."
    local subnets=$(aws ec2 describe-subnets \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'Subnets[].SubnetId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$subnets" ] && [ "$subnets" != "None" ]; then
        for subnet_id in $subnets; do
            log "Removendo Subnet: $subnet_id"
            aws ec2 delete-subnet --subnet-id "$subnet_id" 2>/dev/null || true
        done
    fi
    
    # 7. Finalmente, remover a VPC
    log "Removendo VPC: $vpc_id"
    if aws ec2 delete-vpc --vpc-id "$vpc_id" 2>/dev/null; then
        success "‚úÖ VPC $vpc_id removida com sucesso"
    else
        warn "‚ùå Falha ao remover VPC $vpc_id - pode ter depend√™ncias restantes"
        warn "üí° Verifique manualmente no console AWS"
    fi
}

# Configurar kubectl
configure_kubectl() {
    log "Configurando kubectl..."
    
    verify_credentials
    
    # Tentar obter informa√ß√µes do cluster via terraform
    local cluster_name_tf=""
    local aws_region_tf=""
    
    cd "$TERRAFORM_DIR"
    if terraform output cluster_name >/dev/null 2>&1; then
        cluster_name_tf=$(terraform output -raw cluster_name 2>/dev/null || echo "")
        aws_region_tf=$(terraform output -raw aws_region 2>/dev/null || echo "$AWS_REGION")
    fi
    cd - >/dev/null
    
    # Usar valores do terraform ou padr√£o
    local cluster_to_use="${cluster_name_tf:-$CLUSTER_NAME}"
    local region_to_use="${aws_region_tf:-$AWS_REGION}"
    
    # Verificar se cluster existe
    if ! aws eks describe-cluster --name "$cluster_to_use" --region "$region_to_use" >/dev/null 2>&1; then
        error "Cluster $cluster_to_use n√£o encontrado na regi√£o $region_to_use"
    fi
    
    aws eks update-kubeconfig --region "$region_to_use" --name "$cluster_to_use"
    
    success "kubectl configurado para cluster: $cluster_to_use"
    
    # Verificar conex√£o
    log "Verificando conex√£o com cluster..."
    kubectl get nodes || warn "Falha ao conectar com o cluster"
}

# Configurar acesso ao ECR
setup_ecr_access() {
    log "Configurando secret para ECR..."
    
    # Remover secret existente se houver
    kubectl delete secret ecr-secret -n $APP_NAMESPACE --ignore-not-found=true
    
    # Obter token do ECR
    local ecr_token=$(aws ecr get-login-password --region $AWS_REGION)
    
    # Criar novo secret
    kubectl create secret docker-registry ecr-secret \
        --docker-server=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com \
        --docker-username=AWS \
        --docker-password="$ecr_token" \
        --namespace=$APP_NAMESPACE
    
    success "Secret ECR configurado"
}

# Obter informa√ß√µes da aplica√ß√£o
get_application_info() {
    log "Obtendo informa√ß√µes da aplica√ß√£o..."
    
    local load_balancer=""
    local max_attempts=10
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        
        if [ -n "$load_balancer" ]; then
            break
        fi
        
        info "Aguardando LoadBalancer (tentativa $attempt/$max_attempts)..."
        sleep 15
        attempt=$((attempt + 1))
    done
    
    echo
    echo -e "${GREEN}=====================================${NC}"
    echo -e "${GREEN}üéâ APLICA√á√ÉO DEPLOYADA COM SUCESSO!${NC}"
    echo -e "${GREEN}=====================================${NC}"
    echo
    echo -e "${BLUE}üìä Informa√ß√µes da Aplica√ß√£o:${NC}"
    echo "   Namespace: $APP_NAMESPACE"
    echo "   Deployment: fiap-soat-nestjs"
    echo "   Service: fiap-soat-nestjs-service"
    echo "   Imagem: $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG"
    
    if [ -n "$load_balancer" ]; then
        echo
        echo -e "${CYAN}üåç URLs da Aplica√ß√£o:${NC}"
        echo "   Principal: http://$load_balancer/"
        echo "   Health: http://$load_balancer/health"
        echo
        echo -e "${YELLOW}üß™ Testando aplica√ß√£o...${NC}"
        if curl -s -f "http://$load_balancer/health" >/dev/null; then
            success "‚úÖ Aplica√ß√£o respondendo corretamente!"
        else
            warn "‚ö†Ô∏è  Aguarde alguns minutos para a aplica√ß√£o ficar completamente dispon√≠vel"
        fi
    else
        warn "LoadBalancer ainda n√£o dispon√≠vel. Use: kubectl get svc -n $APP_NAMESPACE"
    fi
    
    echo
    echo -e "${BLUE}üîç Comandos √∫teis:${NC}"
    echo "   kubectl get pods -n $APP_NAMESPACE"
    echo "   kubectl logs -l app=fiap-soat-nestjs -n $APP_NAMESPACE"
    echo "   kubectl describe service fiap-soat-nestjs-service -n $APP_NAMESPACE"
}

# Deploy da aplica√ß√£o
deploy_application() {
    log "üöÄ Fazendo deploy da aplica√ß√£o NestJS..."
    
    verify_credentials
    
    # Verificar se a imagem ECR existe
    log "Verificando imagem no ECR..."
    if ! aws ecr describe-images --region $AWS_REGION --repository-name $ECR_REPOSITORY --image-ids imageTag=$IMAGE_TAG &>/dev/null; then
        error "Imagem $ECR_REPOSITORY:$IMAGE_TAG n√£o encontrada no ECR!"
    fi
    success "Imagem ECR verificada"
    
    # Aplicar namespace
    log "Aplicando namespace..."
    kubectl apply -f $MANIFESTS_DIR/namespace.yaml
    
    # Criar/atualizar secret ECR
    log "Configurando acesso ao ECR..."
    setup_ecr_access
    
    # Aplicar deployment e service
    log "Aplicando manifests da aplica√ß√£o..."
    kubectl apply -f $MANIFESTS_DIR/deployment.yaml
    kubectl apply -f $MANIFESTS_DIR/service.yaml
    
    # Aguardar deployment
    log "Aguardando deployment ficar pronto..."
    kubectl wait --for=condition=available --timeout=300s deployment/fiap-soat-nestjs -n $APP_NAMESPACE
    
    # Aguardar LoadBalancer
    log "Aguardando LoadBalancer ficar dispon√≠vel..."
    sleep 30  # Dar tempo para o LoadBalancer provisionar
    
    # Obter informa√ß√µes do servi√ßo
    get_application_info
    
    success "‚úÖ Aplica√ß√£o NestJS deployada com sucesso!"
}

# Verificar apenas status da aplica√ß√£o
check_application_status() {
    log "üìä Verificando status da aplica√ß√£o..."
    
    if ! kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
        warn "Namespace '$APP_NAMESPACE' n√£o encontrado. Aplica√ß√£o n√£o est√° deployada."
        return 1
    fi
    
    echo
    echo -e "${BLUE}=== üì¶ FIAP SOAT NESTJS APP ===${NC}"
    kubectl get all -n $APP_NAMESPACE 2>/dev/null || echo "‚ùå Erro ao obter recursos da aplica√ß√£o"
    
    echo
    echo -e "${BLUE}=== üîç LOGS RECENTES ===${NC}"
    kubectl logs -l app=fiap-soat-nestjs -n $APP_NAMESPACE --tail=10 2>/dev/null || echo "‚ùå Erro ao obter logs"
    
    echo
    echo -e "${BLUE}=== üåç ACCESS INFO ===${NC}"
    local load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
    
    if [ -n "$load_balancer" ]; then
        echo "   üîó Aplica√ß√£o: http://$load_balancer/"
        echo "   ‚ù§Ô∏è Health: http://$load_balancer/health"
        
        echo
        echo -e "${YELLOW}üß™ Testando conectividade...${NC}"
        if curl -s -f "http://$load_balancer/health" >/dev/null 2>&1; then
            success "‚úÖ Aplica√ß√£o FUNCIONANDO!"
        else
            warn "‚ö†Ô∏è  Aplica√ß√£o n√£o est√° respondendo"
        fi
    else
        warn "LoadBalancer n√£o dispon√≠vel ou ainda sendo provisionado"
    fi
    
    success "Verifica√ß√£o da aplica√ß√£o conclu√≠da!"
}

# Verificar status
check_status() {
    log "üìä Verificando status completo do ambiente..."
    
    verify_credentials
    
    echo
    echo -e "${BLUE}=== üèóÔ∏è  INFRAESTRUTURA AWS ===${NC}"
    
    # Status do cluster EKS
    local cluster_status=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
    if [ "$cluster_status" != "NOT_FOUND" ]; then
        echo "‚úÖ Cluster EKS: $cluster_status"
        
        # Informa√ß√µes do cluster
        local cluster_info=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.{Version:version,Endpoint:endpoint,CreatedAt:createdAt}' --output table 2>/dev/null || true)
        echo "$cluster_info"
        
        # Node groups
        echo
        echo "Node Groups:"
        aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --output table 2>/dev/null || echo "  Nenhum node group encontrado"
        
    else
        echo "‚ùå Cluster EKS: N√ÉO ENCONTRADO"
    fi
    
    echo
    echo -e "${BLUE}=== ‚ò∏Ô∏è  KUBERNETES CLUSTER ===${NC}"
    
    if kubectl cluster-info >/dev/null 2>&1; then
        echo "‚úÖ Conectividade kubectl: OK"
        
        echo
        echo "=== NODES ==="
        kubectl get nodes -o wide 2>/dev/null || echo "‚ùå Erro ao obter nodes"
        
        echo
        echo "=== SYSTEM PODS ==="
        kubectl get pods -n kube-system -o wide 2>/dev/null || echo "‚ùå Erro ao obter pods do sistema"
        
        echo
        echo "=== ALL NAMESPACES ==="
        kubectl get pods -A 2>/dev/null || echo "‚ùå Erro ao obter todos os pods"
        
        echo
        echo "=== SERVICES ==="
        kubectl get svc -A 2>/dev/null || echo "‚ùå Erro ao obter servi√ßos"
        
        # Verificar se aplica√ß√£o est√° deployada
        if kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
            echo
            echo -e "${BLUE}=== üì¶ FIAP SOAT NESTJS APP ===${NC}"
            kubectl get all -n $APP_NAMESPACE 2>/dev/null || echo "‚ùå Erro ao obter recursos da aplica√ß√£o"
            
            echo
            echo -e "${BLUE}=== üåê ACCESS INFO ===${NC}"
            local load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            
            if [ -n "$load_balancer" ]; then
                echo "Aplica√ß√£o dispon√≠vel em:"
                echo "  üîó Principal: http://$load_balancer/"
                echo "  ‚ù§Ô∏è Health: http://$load_balancer/health"
                
                echo
                echo -e "${YELLOW}üß™ Testando conectividade...${NC}"
                if curl -s -f "http://$load_balancer/health" >/dev/null 2>&1; then
                    success "‚úÖ Aplica√ß√£o FUNCIONANDO!"
                else
                    warn "‚ö†Ô∏è  Aplica√ß√£o pode ainda estar inicializando"
                fi
            else
                warn "LoadBalancer ainda n√£o dispon√≠vel"
            fi
        else
            warn "Namespace '$APP_NAMESPACE' n√£o encontrado. Aplica√ß√£o n√£o deployada?"
        fi
        
    else
        echo "‚ùå Conectividade kubectl: FALHOU"
        warn "Configure kubectl com: $0 (op√ß√£o 5)"
    fi
    
    echo
    echo -e "${BLUE}=== üí∞ CUSTO ESTIMADO ===${NC}"
    echo "üìä Recursos ativos que geram custo:"
    echo "   ‚Ä¢ EKS Control Plane: ~$0.10/hora (~$73/m√™s)"
    echo "   ‚Ä¢ Node Group (t3.small): ~$0.0208/hora (~$15/m√™s)"
    echo "   ‚Ä¢ EBS Storage: ~$0.10/GB/m√™s"
    echo "   ‚Ä¢ Data Transfer: vari√°vel"
    echo
    warn "üí° Para evitar custos, execute limpeza completa quando n√£o precisar do ambiente"
    
    success "Verifica√ß√£o de status conclu√≠da!"
}

# Menu principal
main() {
    echo -e "${BLUE}"
    echo "=================================================="
    echo "üéØ FIAP SOAT - EKS Deploy Script v2.0"
    echo "üè´ AWS Academy Optimized"
    echo "üí° Autor: rs94458"
    echo "üîß Recursos: Deploy robusto + Limpeza completa"
    echo "=================================================="
    echo -e "${NC}"
    
    check_dependencies
    check_aws_credentials
    
    echo
    echo
    echo "Escolha uma opcao:"
    echo
    echo -e "${CYAN}üèóÔ∏è  INFRAESTRUTURA:${NC}"
    echo "1) üöÄ Deploy completo (infraestrutura + aplicacao)"
    echo "2) üèóÔ∏è  Apenas infraestrutura EKS"
    echo "3) ‚öôÔ∏è  Configurar kubectl"
    echo
    echo -e "${CYAN}üì¶ APLICACAO NESTJS:${NC}"
    echo "4) üì¶ Deploy aplicacao NestJS"
    echo "5) üßπ Limpar aplicacao"
    echo "6) üìä Status aplicacao"
    echo
    echo -e "${CYAN}üîç MONITORAMENTO:${NC}"
    echo "7) üìä Status completo (infra + app)"
    echo "8) üîç Verificar recursos AWS"
    echo
    echo -e "${CYAN}üßπ LIMPEZA:${NC}"
    echo "9) üßπ Limpeza completa (DESTROY ALL)"
    echo "10) üõ†Ô∏è Limpar recursos orfaos"
    echo "11) üîß Limpar state Terraform"
    echo
    echo "0) üëã Sair"
    echo "0) üëã Sair"
    echo
    
    read -p "Op√ß√£o: " option
    
    case $option in
        1)
            deploy_infrastructure
            configure_kubectl
            deploy_application
            check_status
            ;;
        2)
            deploy_infrastructure
            configure_kubectl
            ;;
        3)
            configure_kubectl
            ;;
        4)
            configure_kubectl
            deploy_application
            ;;
        5)
            cleanup_application
            ;;
        6)
            check_application_status
            ;;
        7)
            configure_kubectl
            check_status
            ;;
        8)
            check_aws_resources
            ;;
        9)
            cleanup_resources
            ;;
        10)
            cleanup_orphaned_resources
            ;;
        11)
            clean_terraform_state
            ;;
        0)
            success "üëã Tchau!"
            exit 0
            ;;
        *)
            error "Op√ß√£o inv√°lida"
            ;;
    esac
}

# Executar script
main
