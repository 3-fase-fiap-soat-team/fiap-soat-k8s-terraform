#!/bin/bash

# Script de deploy para FIAP SOAT - AWS Academy
# Autor: rs94458
# Uso: ./scripts/deploy.sh
# VersÃ£o 2.0 - Com limpeza robusta e deploy automatizado

set -e

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# ConfiguraÃ§Ãµes
CLUSTER_NAME="fiap-soat-cluster"
AWS_REGION="us-east-1"
ACCOUNT_ID="280273007505"
ECR_REPOSITORY="fiap-soat-nestjs-app"
IMAGE_TAG="latest"
APP_NAMESPACE="fiap-soat-app"

# Detectar diretÃ³rio base do projeto
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
TERRAFORM_DIR="$PROJECT_DIR/environments/dev"
MANIFESTS_DIR="$PROJECT_DIR/manifests"

MAX_RETRIES=3
CREDENTIAL_CHECK_INTERVAL=300 # 5 minutos

# FunÃ§Ã£o de log
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[WARNING] $1${NC}"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}"
    exit 1
}

info() {
    echo -e "${BLUE}[INFO] $1${NC}"
}

success() {
    echo -e "${GREEN}[SUCCESS] âœ… $1${NC}"
}

# Verificar dependÃªncias
check_dependencies() {
    log "Verificando dependÃªncias..."
    
    command -v terraform >/dev/null 2>&1 || error "Terraform nÃ£o encontrado. Instale: https://terraform.io/"
    command -v kubectl >/dev/null 2>&1 || error "kubectl nÃ£o encontrado. Instale: https://kubernetes.io/docs/tasks/tools/"
    command -v aws >/dev/null 2>&1 || error "AWS CLI nÃ£o encontrado. Instale: https://aws.amazon.com/cli/"
    
    success "DependÃªncias verificadas"
}

# Verificar e renovar credenciais AWS
check_aws_credentials() {
    log "Verificando credenciais AWS..."
    
    local retry_count=0
    while [ $retry_count -lt $MAX_RETRIES ]; do
        if aws sts get-caller-identity >/dev/null 2>&1; then
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            USER_ARN=$(aws sts get-caller-identity --query Arn --output text)
            
            # Verificar se sÃ£o credenciais temporÃ¡rias (AWS Academy)
            if echo "$USER_ARN" | grep -q "assumed-role"; then
                info "Credenciais temporÃ¡rias detectadas (AWS Academy)"
                
                # Verificar tempo restante (aproximado)
                local session_token=$(aws configure get aws_session_token)
                if [ -n "$session_token" ]; then
                    warn "â° Lembre-se: credenciais AWS Academy expiram em ~3h"
                fi
            fi
            
            success "Conectado Ã  conta AWS: $ACCOUNT_ID"
            return 0
        else
            retry_count=$((retry_count + 1))
            warn "Falha na verificaÃ§Ã£o de credenciais (tentativa $retry_count/$MAX_RETRIES)"
            
            if [ $retry_count -lt $MAX_RETRIES ]; then
                info "ğŸ’¡ Configure novas credenciais com: ./scripts/aws-config.sh"
                read -p "Pressione Enter apÃ³s configurar as credenciais ou Ctrl+C para cancelar..."
            fi
        fi
    done
    
    error "Credenciais AWS invÃ¡lidas apÃ³s $MAX_RETRIES tentativas"
}

# Verificar se credenciais ainda estÃ£o vÃ¡lidas
verify_credentials() {
    if ! aws sts get-caller-identity >/dev/null 2>&1; then
        warn "ğŸ”„ Credenciais expiraram durante a operaÃ§Ã£o"
        info "ğŸ’¡ Renove as credenciais com: ./scripts/aws-config.sh"
        read -p "Pressione Enter apÃ³s renovar as credenciais..."
        check_aws_credentials
    fi
}

# Limpar state Ã³rfÃ£o e inconsistente
clean_terraform_state() {
    log "ğŸ§¹ Limpando estado do Terraform..."
    
    cd "$TERRAFORM_DIR"
    
    # Verificar se hÃ¡ state file
    if [ ! -f "terraform.tfstate" ]; then
        info "Nenhum state file encontrado - deploy limpo"
        check_orphaned_resources_before_deploy
    else
        # Verificar consistÃªncia do state com AWS
        check_state_consistency
    fi
    
    # Limpar planos e backups Ã³rfÃ£os
    rm -f tfplan terraform.tfstate.backup.drift-* 2>/dev/null || true
    
    success "State limpo"
    cd - >/dev/null
}

# Verificar recursos Ã³rfÃ£os antes do deploy
check_orphaned_resources_before_deploy() {
    log "ğŸ” Verificando recursos Ã³rfÃ£os antes do deploy..."
    
    # Verificar VPCs Ã³rfÃ£s do projeto
    local vpc_ids=$(aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].VpcId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$vpc_ids" ] && [ "$vpc_ids" != "None" ]; then
        warn "ğŸš¨ Detectadas VPCs Ã³rfÃ£s do projeto: $vpc_ids"
        echo
        echo "OpÃ§Ãµes disponÃ­veis:"
        echo "1) ğŸ—‘ï¸  Remover VPCs Ã³rfÃ£s (recomendado para deploy limpo)"
        echo "2) ğŸ”„ Tentar reutilizar VPCs existentes (arriscado)"
        echo "3) â­ï¸  Continuar e deixar o Terraform decidir"
        echo
        read -p "Escolha (1-3): " vpc_choice
        
        case $vpc_choice in
            1)
                log "Removendo VPCs Ã³rfÃ£s antes do deploy..."
                for vpc_id in $vpc_ids; do
                    cleanup_single_vpc "$vpc_id"
                done
                ;;
            2)
                warn "âš ï¸  ARRISCADO: Terraform pode dar conflito ou erro"
                warn "ğŸ’¡ Monitore o plano do Terraform cuidadosamente"
                ;;
            3)
                info "Continuando com deploy - Terraform tentarÃ¡ resolver conflitos"
                warn "âš ï¸  Se houver erro, execute limpeza de Ã³rfÃ£os (opÃ§Ã£o 8 do menu)"
                ;;
            *)
                error "OpÃ§Ã£o invÃ¡lida"
                ;;
        esac
    fi
    
    # Verificar outros recursos Ã³rfÃ£os
    local clusters=$(aws eks list-clusters --query "clusters[?contains(@, \`$CLUSTER_NAME\`)]" --output text 2>/dev/null || true)
    if [ -n "$clusters" ] && [ "$clusters" != "None" ]; then
        warn "ğŸš¨ Cluster EKS Ã³rfÃ£o detectado: $clusters"
        warn "âš ï¸  Terraform pode dar erro ao tentar criar cluster com mesmo nome"
        echo
        read -p "Remover cluster Ã³rfÃ£o antes do deploy? (s/N): " remove_cluster
        
        if [[ "$remove_cluster" =~ ^[Ss]$ ]]; then
            aws eks delete-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" || true
            log "Aguardando remoÃ§Ã£o do cluster..."
            sleep 60
        fi
    fi
}

# Verificar consistÃªncia do state com AWS  
check_state_consistency() {
    log "ğŸ” Verificando consistÃªncia do state com AWS..."
    
    # Verificar se recursos no state ainda existem no AWS
    local resources=$(terraform state list 2>/dev/null || true)
    
    if [ -n "$resources" ]; then
        echo "$resources" | while read resource; do
            case $resource in
                *aws_eks_cluster*)
                    local cluster_name_state=$(terraform state show "$resource" 2>/dev/null | grep "name.*=" | awk '{print $3}' | tr -d '"' || true)
                    if [ -n "$cluster_name_state" ]; then
                        if ! aws eks describe-cluster --name "$cluster_name_state" --region "$AWS_REGION" >/dev/null 2>&1; then
                            warn "Cluster '$cluster_name_state' no state mas nÃ£o existe no AWS"
                            warn "Removendo do state: $resource"
                            terraform state rm "$resource" || true
                        fi
                    fi
                    ;;
                *aws_vpc*)
                    local vpc_id_state=$(terraform state show "$resource" 2>/dev/null | grep "id.*=" | awk '{print $3}' | tr -d '"' || true)
                    if [ -n "$vpc_id_state" ]; then
                        if ! aws ec2 describe-vpcs --vpc-ids "$vpc_id_state" >/dev/null 2>&1; then
                            warn "VPC '$vpc_id_state' no state mas nÃ£o existe no AWS"
                            warn "Removendo do state: $resource"
                            terraform state rm "$resource" || true
                        fi
                    fi
                    ;;
            esac
        done
    fi
}

# Deploy robusto da infraestrutura
deploy_infrastructure() {
    log "ğŸš€ Iniciando deploy da infraestrutura..."
    
    cd "$TERRAFORM_DIR"
    
    # Verificar credenciais antes de comeÃ§ar
    verify_credentials
    
    # Limpar state inconsistente
    clean_terraform_state
    
    # Inicializar Terraform
    log "Inicializando Terraform..."
    terraform init
    
    # Validar configuraÃ§Ã£o
    log "Validando configuraÃ§Ã£o..."
    terraform validate
    
    # Mostrar plano e detectar conflitos
    log "Criando plano de execuÃ§Ã£o..."
    if ! terraform plan -out=tfplan; then
        error "âŒ Falha no plano do Terraform!"
        warn "ğŸ’¡ PossÃ­veis causas:"
        warn "   â€¢ Recursos Ã³rfÃ£os conflitando"
        warn "   â€¢ CIDR blocks duplicados" 
        warn "   â€¢ Tags conflitantes"
        warn "   â€¢ Nomes de recursos jÃ¡ existentes"
        echo
        warn "ğŸ”§ SoluÃ§Ãµes:"
        warn "   1) Execute: ./scripts/deploy.sh (opÃ§Ã£o 8 - Limpar Ã³rfÃ£os)"
        warn "   2) Execute: ./scripts/deploy.sh (opÃ§Ã£o 9 - Limpar state)"
        warn "   3) Verifique manualmente no console AWS"
        exit 1
    fi
    
    # Verificar se o plano indica recursos Ã³rfÃ£os sendo importados/conflitados
    log "Analisando plano para detectar conflitos reais..."
    if terraform show tfplan | grep -q "will be imported\|already exists" | grep -v "resolve_conflicts"; then
        warn "âš ï¸  Detectados possÃ­veis conflitos no plano:"
        terraform show tfplan | grep -A 2 -B 2 "will be imported\|already exists" | grep -v "resolve_conflicts" || true
        echo
        read -p "Continuar mesmo assim? (s/N): " continue_with_conflicts
        
        if [[ ! "$continue_with_conflicts" =~ ^[Ss]$ ]]; then
            error "Deploy cancelado devido a conflitos detectados"
        fi
    else
        info "Plano validado - nenhum conflito real detectado"
    fi
    
    # Confirmar aplicaÃ§Ã£o
    warn "âš ï¸  ATENÃ‡ÃƒO: Este deploy criarÃ¡ recursos que CUSTAM DINHEIRO na AWS!"
    warn "ğŸ’° EKS Control Plane: ~$73/mÃªs"
    warn "ğŸ’° Node Group (t3.small): ~$15/mÃªs"
    warn "ğŸ’° Total estimado: ~$88/mÃªs (pode estourar budget AWS Academy!)"
    echo
    read -p "Deseja continuar? (digite 'sim' para confirmar): " confirm
    
    if [ "$confirm" != "sim" ]; then
        error "Deploy cancelado pelo usuÃ¡rio"
    fi
    
    # Aplicar mudanÃ§as com retry em caso de expiraÃ§Ã£o de credenciais
    local apply_success=false
    local retry_count=0
    
    while [ $retry_count -lt $MAX_RETRIES ] && [ "$apply_success" = false ]; do
        log "Aplicando mudanÃ§as (tentativa $((retry_count + 1))/$MAX_RETRIES)..."
        
        if terraform apply tfplan; then
            apply_success=true
            success "Infraestrutura criada com sucesso!"
        else
            retry_count=$((retry_count + 1))
            
            if [ $retry_count -lt $MAX_RETRIES ]; then
                warn "Falha no apply. Verificando se foi problema de credenciais..."
                verify_credentials
                
                # Recriar plano com credenciais renovadas
                log "Recriando plano com credenciais renovadas..."
                terraform plan -out=tfplan
            fi
        fi
    done
    
    if [ "$apply_success" = false ]; then
        error "Falha no deploy apÃ³s $MAX_RETRIES tentativas"
    fi
    
    # Aguardar cluster ficar ativo
    wait_for_cluster_ready
    
    cd - >/dev/null
}

# Aguardar cluster ficar pronto
wait_for_cluster_ready() {
    log "â³ Aguardando cluster ficar ACTIVE..."
    
    local max_wait=900  # 15 minutos
    local wait_time=0
    local check_interval=30
    
    while [ $wait_time -lt $max_wait ]; do
        verify_credentials  # Verificar credenciais a cada check
        
        local status=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$status" = "ACTIVE" ]; then
            success "Cluster estÃ¡ ACTIVE!"
            return 0
        elif [ "$status" = "FAILED" ] || [ "$status" = "NOT_FOUND" ]; then
            error "Cluster falhou ou nÃ£o foi encontrado. Status: $status"
        else
            info "Status do cluster: $status (aguardando...)"
            sleep $check_interval
            wait_time=$((wait_time + check_interval))
        fi
    done
    
    error "Timeout aguardando cluster ficar ativo (${max_wait}s)"
}

# Remover aplicaÃ§Ã£o Kubernetes
cleanup_application() {
    log "ğŸ§¹ Removendo aplicaÃ§Ã£o do Kubernetes..."
    
    if kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
        # Remover todos os recursos do namespace
        kubectl delete all --all -n $APP_NAMESPACE --ignore-not-found=true || true
        
        # Remover secrets especÃ­ficos
        kubectl delete secret ecr-secret -n $APP_NAMESPACE --ignore-not-found=true || true
        
        # Aguardar remoÃ§Ã£o completa dos recursos
        log "Aguardando remoÃ§Ã£o dos recursos..."
        sleep 10
        
        # ForÃ§ar remoÃ§Ã£o do namespace se ainda existir
        kubectl delete namespace $APP_NAMESPACE --ignore-not-found=true || true
        
        success "AplicaÃ§Ã£o removida completamente"
    else
        info "Namespace '$APP_NAMESPACE' nÃ£o encontrado"
    fi
}

# Verificar recursos AWS restantes
check_aws_resources() {
    log "ğŸ“Š Verificando recursos AWS restantes..."
    
    verify_credentials
    
    echo
    echo -e "${BLUE}=== ğŸ—ï¸  EKS CLUSTERS ===${NC}"
    aws eks list-clusters --query "clusters[?contains(@, \`$CLUSTER_NAME\`)]" --output table 2>/dev/null || echo "Nenhum cluster encontrado"
    
    echo
    echo -e "${BLUE}=== ğŸ–¥ï¸  EC2 INSTANCES ===${NC}"
    aws ec2 describe-instances \
        --filters "Name=tag:Project,Values=fiap-soat*" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
        --query 'Reservations[].Instances[].{ID:InstanceId,State:State.Name,Type:InstanceType,Name:Tags[?Key==`Name`]|[0].Value}' \
        --output table 2>/dev/null || echo "Nenhuma instÃ¢ncia encontrada"
    
    echo
    echo -e "${BLUE}=== âš–ï¸  LOAD BALANCERS ===${NC}"
    aws elbv2 describe-load-balancers \
        --query "LoadBalancers[?contains(LoadBalancerName, \`fiap-soat\`)].{Name:LoadBalancerName,State:State.Code,Type:Type}" \
        --output table 2>/dev/null || echo "Nenhum load balancer encontrado"
    
    echo
    echo -e "${BLUE}=== ğŸŒ VPCs ===${NC}"
    aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].{ID:VpcId,CIDR:CidrBlock,State:State,Name:Tags[?Key==`Name`]|[0].Value}' \
        --output table 2>/dev/null || echo "Nenhuma VPC encontrada"
    
    echo
    echo -e "${BLUE}=== ğŸ’¾ VOLUMES EBS ===${NC}"
    aws ec2 describe-volumes \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Volumes[].{ID:VolumeId,Size:Size,State:State,Type:VolumeType}' \
        --output table 2>/dev/null || echo "Nenhum volume encontrado"
}

# Limpeza completa de recursos
cleanup_resources() {
    log "ğŸ§¹ Iniciando limpeza completa de recursos..."
    
    # Mostrar recursos antes da destruiÃ§Ã£o
    info "Verificando recursos que serÃ£o destruÃ­dos..."
    check_aws_resources
    
    echo
    # Confirmar limpeza
    warn "âš ï¸  ATENÃ‡ÃƒO: Isso irÃ¡ DESTRUIR TODOS os recursos AWS criados!"
    warn "ğŸ’€ Cluster EKS, Node Groups, VPC, Load Balancers, etc."
    warn "ğŸ’° Isso irÃ¡ PARAR TODOS OS CUSTOS AWS!"
    warn "ğŸ”¥ Esta aÃ§Ã£o Ã© IRREVERSÃVEL!"
    echo
    read -p "Tem certeza? Digite 'DESTRUIR' para confirmar: " confirm
    
    if [ "$confirm" != "DESTRUIR" ]; then
        info "Limpeza cancelada pelo usuÃ¡rio"
        warn "ğŸ’° Lembre-se: recursos AWS continuam gerando custos!"
        return 0
    fi
    
    # Remover aplicaÃ§Ã£o primeiro
    cleanup_application
    
    cd "$TERRAFORM_DIR"
    verify_credentials
    
    # Mostrar plano de destroy
    log "Criando plano de destruiÃ§Ã£o..."
    terraform plan -destroy -out=tfplan-destroy
    
    # Tentar destroy normal primeiro
    log "Executando destruiÃ§Ã£o..."
    if terraform apply tfplan-destroy; then
        success "Destroy executado com sucesso!"
    else
        warn "Destroy falhou. Tentando limpeza forÃ§ada..."
        force_cleanup
    fi
    
    # Limpar contexto kubectl
    cleanup_kubectl_context
    
    # Limpar state e arquivos temporÃ¡rios
    cleanup_local_files
    
    # Verificar se restaram recursos Ã³rfÃ£os
    log "Verificando recursos Ã³rfÃ£os..."
    check_aws_resources
    
    success "ğŸ‰ Limpeza completa finalizada!"
    success "ğŸ’° Recursos AWS removidos - custos parados!"
    cd - >/dev/null
}

# Limpar contexto kubectl
cleanup_kubectl_context() {
    log "ğŸ§¹ Limpando contexto kubectl..."
    
    if command -v kubectl >/dev/null 2>&1; then
        local current_context=$(kubectl config current-context 2>/dev/null || echo "")
        if [[ "$current_context" == *"fiap-soat"* ]]; then
            kubectl config delete-context "$current_context" 2>/dev/null || true
            kubectl config delete-cluster "$CLUSTER_NAME" 2>/dev/null || true
            success "Contexto kubectl removido"
        fi
    fi
}

# Limpar arquivos locais
cleanup_local_files() {
    log "ğŸ§¹ Limpando arquivos temporÃ¡rios..."
    
    # Remover arquivos terraform
    rm -f terraform.tfstate* tfplan* .terraform.lock.hcl 2>/dev/null || true
    rm -rf .terraform/ 2>/dev/null || true
    
    # Remover backups antigos
    find . -name "terraform.tfstate.backup*" -delete 2>/dev/null || true
    find . -name "tfplan*" -delete 2>/dev/null || true
    
    success "Arquivos temporÃ¡rios limpos"
}

# Limpeza forÃ§ada para casos extremos
force_cleanup() {
    warn "âš ï¸  Executando limpeza forÃ§ada de recursos Ã³rfÃ£os..."
    
    # Confirmar limpeza forÃ§ada
    echo
    warn "ğŸ”¥ LIMPEZA FORÃ‡ADA: Tentativa de remover recursos Ã³rfÃ£os individualmente"
    warn "âš ï¸  Isso pode deixar alguns recursos Ã³rfÃ£os que custam dinheiro!"
    read -p "Continuar com limpeza forÃ§ada? (s/N): " force_confirm
    
    if [[ ! "$force_confirm" =~ ^[Ss]$ ]]; then
        info "Limpeza forÃ§ada cancelada"
        return 0
    fi
    
    # Listar recursos que ainda existem
    local resources=$(terraform state list 2>/dev/null || true)
    
    if [ -n "$resources" ]; then
        log "Removendo recursos individualmente..."
        
        # Remover node groups primeiro (dependÃªncias crÃ­ticas)
        echo "$resources" | grep "aws_eks_node_group" | while read resource; do
            log "Removendo node group: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 10
        done
        
        # Remover addons EKS
        echo "$resources" | grep "aws_eks_addon" | while read resource; do
            log "Removendo addon: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 5
        done
        
        # Aguardar node groups serem removidos
        log "Aguardando remoÃ§Ã£o de node groups..."
        sleep 30
        
        # Remover cluster
        echo "$resources" | grep "aws_eks_cluster" | while read resource; do
            log "Removendo cluster: $resource"
            terraform destroy -target="$resource" -auto-approve || true
            sleep 10
        done
        
        # Aguardar cluster ser removido
        log "Aguardando remoÃ§Ã£o do cluster..."
        sleep 60
        
        # Tentar destroy completo final
        log "Tentando destroy completo final..."
        terraform destroy -auto-approve || warn "Alguns recursos podem nÃ£o ter sido removidos"
    fi
    
    # Limpeza manual de recursos Ã³rfÃ£os
    cleanup_orphaned_resources
    
    # Verificar recursos restantes
    log "Verificando recursos restantes..."
    check_aws_resources
    
    warn "ğŸš¨ IMPORTANTE: Verifique manualmente no console AWS se hÃ¡ recursos Ã³rfÃ£os:"
    warn "   â€¢ EC2 â†’ InstÃ¢ncias"
    warn "   â€¢ EKS â†’ Clusters"  
    warn "   â€¢ VPC â†’ Suas VPCs"
    warn "   â€¢ EC2 â†’ Load Balancers"
    warn "ğŸ’° Recursos Ã³rfÃ£os podem continuar gerando custos!"
}

# Limpeza de recursos Ã³rfÃ£os via AWS CLI
cleanup_orphaned_resources() {
    log "ğŸ” Verificando recursos Ã³rfÃ£os via AWS CLI..."
    
    verify_credentials
    
    # Verificar e remover node groups Ã³rfÃ£os primeiro
    log "Verificando node groups..."
    local node_groups=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'nodegroups[]' --output text 2>/dev/null || true)
    
    if [ -n "$node_groups" ] && [ "$node_groups" != "None" ]; then
        warn "ğŸš¨ Node groups Ã³rfÃ£os detectados: $node_groups"
        for ng in $node_groups; do
            log "Removendo node group: $ng"
            aws eks delete-nodegroup \
                --cluster-name "$CLUSTER_NAME" \
                --nodegroup-name "$ng" \
                --region "$AWS_REGION" 2>/dev/null || true
        done
        
        # Aguardar remoÃ§Ã£o dos node groups
        log "Aguardando remoÃ§Ã£o dos node groups..."
        sleep 60
    fi
    
    # Verificar e remover addons Ã³rfÃ£os
    log "Verificando addons EKS..."
    local addons=$(aws eks list-addons --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'addons[]' --output text 2>/dev/null || true)
    
    if [ -n "$addons" ] && [ "$addons" != "None" ]; then
        warn "ğŸš¨ Addons Ã³rfÃ£os detectados: $addons"
        for addon in $addons; do
            log "Removendo addon: $addon"
            aws eks delete-addon \
                --cluster-name "$CLUSTER_NAME" \
                --addon-name "$addon" \
                --region "$AWS_REGION" 2>/dev/null || true
        done
    fi
    
    # Verificar e remover cluster Ã³rfÃ£o
    log "Verificando cluster EKS..."
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
        warn "ğŸš¨ Cluster Ã³rfÃ£o detectado: $CLUSTER_NAME"
        log "Removendo cluster..."
        aws eks delete-cluster \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" 2>/dev/null || true
        
        # Aguardar remoÃ§Ã£o do cluster
        log "Aguardando remoÃ§Ã£o do cluster..."
        sleep 90
    fi
    
    # Verificar instÃ¢ncias EC2 Ã³rfÃ£s do projeto
    log "Verificando instÃ¢ncias EC2..."
    local instances=$(aws ec2 describe-instances \
        --filters "Name=tag:Project,Values=fiap-soat*" "Name=instance-state-name,Values=running,pending" \
        --query 'Reservations[].Instances[].InstanceId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$instances" ] && [ "$instances" != "None" ]; then
        warn "ğŸš¨ InstÃ¢ncias EC2 Ã³rfÃ£s detectadas: $instances"
        warn "âš ï¸  Considere remover manualmente no console AWS"
    fi
    
    # Verificar e limpar VPCs Ã³rfÃ£s
    cleanup_orphaned_vpcs
    
    success "âœ… VerificaÃ§Ã£o de recursos Ã³rfÃ£os concluÃ­da"
}

# Limpeza especÃ­fica de VPCs Ã³rfÃ£s
cleanup_orphaned_vpcs() {
    log "ğŸ” Verificando VPCs Ã³rfÃ£s do projeto..."
    
    # Buscar VPCs do projeto
    local vpc_ids=$(aws ec2 describe-vpcs \
        --filters "Name=tag:Project,Values=fiap-soat*" \
        --query 'Vpcs[].VpcId' \
        --output text 2>/dev/null || true)
    
    if [ -z "$vpc_ids" ] || [ "$vpc_ids" = "None" ]; then
        info "Nenhuma VPC Ã³rfÃ£ do projeto encontrada"
        return 0
    fi
    
    warn "ğŸš¨ VPCs Ã³rfÃ£s detectadas: $vpc_ids"
    echo
    read -p "Deseja remover as VPCs Ã³rfÃ£s? (s/N): " vpc_confirm
    
    if [[ ! "$vpc_confirm" =~ ^[Ss]$ ]]; then
        info "Limpeza de VPCs cancelada"
        return 0
    fi
    
    for vpc_id in $vpc_ids; do
        log "ğŸ§¹ Limpando VPC: $vpc_id"
        cleanup_single_vpc "$vpc_id"
    done
}

# Limpar uma VPC especÃ­fica e suas dependÃªncias
cleanup_single_vpc() {
    local vpc_id="$1"
    
    if [ -z "$vpc_id" ]; then
        error "VPC ID nÃ£o fornecido"
    fi
    
    log "Removendo dependÃªncias da VPC $vpc_id..."
    
    # 1. Remover instÃ¢ncias EC2 (se houver)
    log "Verificando instÃ¢ncias EC2 na VPC..."
    local instances=$(aws ec2 describe-instances \
        --filters "Name=vpc-id,Values=$vpc_id" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
        --query 'Reservations[].Instances[].InstanceId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$instances" ] && [ "$instances" != "None" ]; then
        warn "Terminando instÃ¢ncias: $instances"
        aws ec2 terminate-instances --instance-ids $instances 2>/dev/null || true
        
        # Aguardar terminaÃ§Ã£o
        log "Aguardando terminaÃ§Ã£o das instÃ¢ncias..."
        aws ec2 wait instance-terminated --instance-ids $instances 2>/dev/null || true
    fi
    
    # 2. Remover NAT Gateways
    log "Removendo NAT Gateways..."
    local nat_gateways=$(aws ec2 describe-nat-gateways \
        --filter "Name=vpc-id,Values=$vpc_id" \
        --query 'NatGateways[?State==`available`].NatGatewayId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$nat_gateways" ] && [ "$nat_gateways" != "None" ]; then
        for nat_id in $nat_gateways; do
            log "Removendo NAT Gateway: $nat_id"
            aws ec2 delete-nat-gateway --nat-gateway-id "$nat_id" 2>/dev/null || true
        done
        
        # Aguardar NAT Gateways serem removidos
        log "Aguardando remoÃ§Ã£o dos NAT Gateways..."
        sleep 30
    fi
    
    # 3. Remover Internet Gateway
    log "Removendo Internet Gateway..."
    local igw_id=$(aws ec2 describe-internet-gateways \
        --filters "Name=attachment.vpc-id,Values=$vpc_id" \
        --query 'InternetGateways[].InternetGatewayId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$igw_id" ] && [ "$igw_id" != "None" ]; then
        log "Detaching e removendo Internet Gateway: $igw_id"
        aws ec2 detach-internet-gateway --internet-gateway-id "$igw_id" --vpc-id "$vpc_id" 2>/dev/null || true
        sleep 5
        aws ec2 delete-internet-gateway --internet-gateway-id "$igw_id" 2>/dev/null || true
    fi
    
    # 4. Remover Route Tables (nÃ£o default e nÃ£o associadas)
    log "Removendo Route Tables..."
    local route_tables=$(aws ec2 describe-route-tables \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$route_tables" ] && [ "$route_tables" != "None" ]; then
        for rt_id in $route_tables; do
            log "Limpando rotas da Route Table: $rt_id"
            
            # Primeiro, remover rotas customizadas (nÃ£o locais)
            local routes=$(aws ec2 describe-route-tables \
                --route-table-ids "$rt_id" \
                --query 'RouteTables[0].Routes[?GatewayId!=`local`].DestinationCidrBlock' \
                --output text 2>/dev/null || true)
                
            if [ -n "$routes" ] && [ "$routes" != "None" ]; then
                for cidr in $routes; do
                    log "Removendo rota $cidr da Route Table $rt_id"
                    aws ec2 delete-route --route-table-id "$rt_id" --destination-cidr-block "$cidr" 2>/dev/null || true
                done
            fi
            
            # Aguardar um pouco
            sleep 2
            
            log "Removendo Route Table: $rt_id"
            aws ec2 delete-route-table --route-table-id "$rt_id" 2>/dev/null || true
        done
    fi
    
    # 5. Remover Security Groups (nÃ£o default)
    log "Removendo Security Groups..."
    local security_groups=$(aws ec2 describe-security-groups \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$security_groups" ] && [ "$security_groups" != "None" ]; then
        for sg_id in $security_groups; do
            log "Removendo Security Group: $sg_id"
            aws ec2 delete-security-group --group-id "$sg_id" 2>/dev/null || true
        done
    fi
    
    # 6. Remover Subnets
    log "Removendo Subnets..."
    local subnets=$(aws ec2 describe-subnets \
        --filters "Name=vpc-id,Values=$vpc_id" \
        --query 'Subnets[].SubnetId' \
        --output text 2>/dev/null || true)
    
    if [ -n "$subnets" ] && [ "$subnets" != "None" ]; then
        for subnet_id in $subnets; do
            log "Removendo Subnet: $subnet_id"
            aws ec2 delete-subnet --subnet-id "$subnet_id" 2>/dev/null || true
        done
    fi
    
    # 7. Finalmente, remover a VPC
    log "Removendo VPC: $vpc_id"
    if aws ec2 delete-vpc --vpc-id "$vpc_id" 2>/dev/null; then
        success "âœ… VPC $vpc_id removida com sucesso"
    else
        warn "âŒ Falha ao remover VPC $vpc_id - pode ter dependÃªncias restantes"
        warn "ğŸ’¡ Verifique manualmente no console AWS"
    fi
}

# Configurar kubectl
configure_kubectl() {
    log "Configurando kubectl..."
    
    verify_credentials
    
    # Tentar obter informaÃ§Ãµes do cluster via terraform
    local cluster_name_tf=""
    local aws_region_tf=""
    
    cd "$TERRAFORM_DIR"
    if terraform output cluster_name >/dev/null 2>&1; then
        cluster_name_tf=$(terraform output -raw cluster_name 2>/dev/null || echo "")
        aws_region_tf=$(terraform output -raw aws_region 2>/dev/null || echo "$AWS_REGION")
    fi
    cd - >/dev/null
    
    # Usar valores do terraform ou padrÃ£o
    local cluster_to_use="${cluster_name_tf:-$CLUSTER_NAME}"
    local region_to_use="${aws_region_tf:-$AWS_REGION}"
    
    # Verificar se cluster existe
    if ! aws eks describe-cluster --name "$cluster_to_use" --region "$region_to_use" >/dev/null 2>&1; then
        error "Cluster $cluster_to_use nÃ£o encontrado na regiÃ£o $region_to_use"
    fi
    
    aws eks update-kubeconfig --region "$region_to_use" --name "$cluster_to_use"
    
    success "kubectl configurado para cluster: $cluster_to_use"
    
    # Verificar conexÃ£o
    log "Verificando conexÃ£o com cluster..."
    kubectl get nodes || warn "Falha ao conectar com o cluster"
}

# Configurar acesso ao ECR
setup_ecr_access() {
    log "Configurando secret para ECR..."
    
    # Remover secret existente se houver
    kubectl delete secret ecr-secret -n $APP_NAMESPACE --ignore-not-found=true
    
    # Obter token do ECR
    local ecr_token=$(aws ecr get-login-password --region $AWS_REGION)
    
    # Criar novo secret
    kubectl create secret docker-registry ecr-secret \
        --docker-server=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com \
        --docker-username=AWS \
        --docker-password="$ecr_token" \
        --namespace=$APP_NAMESPACE
    
    success "Secret ECR configurado"
}

# Obter informaÃ§Ãµes da aplicaÃ§Ã£o
get_application_info() {
    log "Obtendo informaÃ§Ãµes da aplicaÃ§Ã£o..."
    
    local load_balancer=""
    local max_attempts=10
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        
        if [ -n "$load_balancer" ]; then
            break
        fi
        
        info "Aguardando LoadBalancer (tentativa $attempt/$max_attempts)..."
        sleep 15
        attempt=$((attempt + 1))
    done
    
    echo
    echo -e "${GREEN}=====================================${NC}"
    echo -e "${GREEN}ğŸ‰ APLICAÃ‡ÃƒO DEPLOYADA COM SUCESSO!${NC}"
    echo -e "${GREEN}=====================================${NC}"
    echo
    echo -e "${BLUE}ğŸ“Š InformaÃ§Ãµes da AplicaÃ§Ã£o:${NC}"
    echo "   Namespace: $APP_NAMESPACE"
    echo "   Deployment: fiap-soat-nestjs"
    echo "   Service: fiap-soat-nestjs-service"
    echo "   Imagem: $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG"
    
    if [ -n "$load_balancer" ]; then
        echo
        echo -e "${CYAN}ğŸŒ URLs da AplicaÃ§Ã£o:${NC}"
        echo "   Principal: http://$load_balancer/"
        echo "   Health: http://$load_balancer/health"
        echo
        echo -e "${YELLOW}ğŸ§ª Testando aplicaÃ§Ã£o...${NC}"
        if curl -s -f "http://$load_balancer/health" >/dev/null; then
            success "âœ… AplicaÃ§Ã£o respondendo corretamente!"
        else
            warn "âš ï¸  Aguarde alguns minutos para a aplicaÃ§Ã£o ficar completamente disponÃ­vel"
        fi
    else
        warn "LoadBalancer ainda nÃ£o disponÃ­vel. Use: kubectl get svc -n $APP_NAMESPACE"
    fi
    
    echo
    echo -e "${BLUE}ğŸ” Comandos Ãºteis:${NC}"
    echo "   kubectl get pods -n $APP_NAMESPACE"
    echo "   kubectl logs -l app=fiap-soat-nestjs -n $APP_NAMESPACE"
    echo "   kubectl describe service fiap-soat-nestjs-service -n $APP_NAMESPACE"
}

# Deploy da aplicaÃ§Ã£o
deploy_application() {
    log "ğŸš€ Fazendo deploy da aplicaÃ§Ã£o NestJS..."
    
    verify_credentials
    
    # Verificar se a imagem ECR existe
    log "Verificando imagem no ECR..."
    if ! aws ecr describe-images --region $AWS_REGION --repository-name $ECR_REPOSITORY --image-ids imageTag=$IMAGE_TAG &>/dev/null; then
        error "Imagem $ECR_REPOSITORY:$IMAGE_TAG nÃ£o encontrada no ECR!"
    fi
    success "Imagem ECR verificada"
    
    # Aplicar namespace
    log "Aplicando namespace..."
    kubectl apply -f $MANIFESTS_DIR/namespace.yaml
    
    # Criar/atualizar secret ECR
    log "Configurando acesso ao ECR..."
    setup_ecr_access
    
    # Aplicar deployment e service
    log "Aplicando manifests da aplicaÃ§Ã£o..."
    kubectl apply -f $MANIFESTS_DIR/deployment.yaml
    kubectl apply -f $MANIFESTS_DIR/service.yaml
    
    # Aguardar deployment
    log "Aguardando deployment ficar pronto..."
    kubectl wait --for=condition=available --timeout=300s deployment/fiap-soat-nestjs -n $APP_NAMESPACE
    
    # Aguardar LoadBalancer
    log "Aguardando LoadBalancer ficar disponÃ­vel..."
    sleep 30  # Dar tempo para o LoadBalancer provisionar
    
    # Obter informaÃ§Ãµes do serviÃ§o
    get_application_info
    
    success "âœ… AplicaÃ§Ã£o NestJS deployada com sucesso!"
}

# Verificar apenas status da aplicaÃ§Ã£o
check_application_status() {
    log "ğŸ“Š Verificando status da aplicaÃ§Ã£o..."
    
    if ! kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
        warn "Namespace '$APP_NAMESPACE' nÃ£o encontrado. AplicaÃ§Ã£o nÃ£o estÃ¡ deployada."
        return 1
    fi
    
    echo
    echo -e "${BLUE}=== ğŸ“¦ FIAP SOAT NESTJS APP ===${NC}"
    kubectl get all -n $APP_NAMESPACE 2>/dev/null || echo "âŒ Erro ao obter recursos da aplicaÃ§Ã£o"
    
    echo
    echo -e "${BLUE}=== ğŸ” LOGS RECENTES ===${NC}"
    kubectl logs -l app=fiap-soat-nestjs -n $APP_NAMESPACE --tail=10 2>/dev/null || echo "âŒ Erro ao obter logs"
    
    echo
    echo -e "${BLUE}=== ğŸŒ ACCESS INFO ===${NC}"
    local load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
    
    if [ -n "$load_balancer" ]; then
        echo "   ğŸ”— AplicaÃ§Ã£o: http://$load_balancer/"
        echo "   â¤ï¸ Health: http://$load_balancer/health"
        
        echo
        echo -e "${YELLOW}ğŸ§ª Testando conectividade...${NC}"
        if curl -s -f "http://$load_balancer/health" >/dev/null 2>&1; then
            success "âœ… AplicaÃ§Ã£o FUNCIONANDO!"
        else
            warn "âš ï¸  AplicaÃ§Ã£o nÃ£o estÃ¡ respondendo"
        fi
    else
        warn "LoadBalancer nÃ£o disponÃ­vel ou ainda sendo provisionado"
    fi
    
    success "VerificaÃ§Ã£o da aplicaÃ§Ã£o concluÃ­da!"
}

# Verificar status
check_status() {
    log "ğŸ“Š Verificando status completo do ambiente..."
    
    verify_credentials
    
    echo
    echo -e "${BLUE}=== ğŸ—ï¸  INFRAESTRUTURA AWS ===${NC}"
    
    # Status do cluster EKS
    local cluster_status=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
    if [ "$cluster_status" != "NOT_FOUND" ]; then
        echo "âœ… Cluster EKS: $cluster_status"
        
        # InformaÃ§Ãµes do cluster
        local cluster_info=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.{Version:version,Endpoint:endpoint,CreatedAt:createdAt}' --output table 2>/dev/null || true)
        echo "$cluster_info"
        
        # Node groups
        echo
        echo "Node Groups:"
        aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --output table 2>/dev/null || echo "  Nenhum node group encontrado"
        
    else
        echo "âŒ Cluster EKS: NÃƒO ENCONTRADO"
    fi
    
    echo
    echo -e "${BLUE}=== â˜¸ï¸  KUBERNETES CLUSTER ===${NC}"
    
    if kubectl cluster-info >/dev/null 2>&1; then
        echo "âœ… Conectividade kubectl: OK"
        
        echo
        echo "=== NODES ==="
        kubectl get nodes -o wide 2>/dev/null || echo "âŒ Erro ao obter nodes"
        
        echo
        echo "=== SYSTEM PODS ==="
        kubectl get pods -n kube-system -o wide 2>/dev/null || echo "âŒ Erro ao obter pods do sistema"
        
        echo
        echo "=== ALL NAMESPACES ==="
        kubectl get pods -A 2>/dev/null || echo "âŒ Erro ao obter todos os pods"
        
        echo
        echo "=== SERVICES ==="
        kubectl get svc -A 2>/dev/null || echo "âŒ Erro ao obter serviÃ§os"
        
        # Verificar se aplicaÃ§Ã£o estÃ¡ deployada
        if kubectl get namespace $APP_NAMESPACE >/dev/null 2>&1; then
            echo
            echo -e "${BLUE}=== ğŸ“¦ FIAP SOAT NESTJS APP ===${NC}"
            kubectl get all -n $APP_NAMESPACE 2>/dev/null || echo "âŒ Erro ao obter recursos da aplicaÃ§Ã£o"
            
            echo
            echo -e "${BLUE}=== ğŸŒ ACCESS INFO ===${NC}"
            local load_balancer=$(kubectl get service fiap-soat-nestjs-service -n $APP_NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            
            if [ -n "$load_balancer" ]; then
                echo "AplicaÃ§Ã£o disponÃ­vel em:"
                echo "  ğŸ”— Principal: http://$load_balancer/"
                echo "  â¤ï¸ Health: http://$load_balancer/health"
                
                echo
                echo -e "${YELLOW}ğŸ§ª Testando conectividade...${NC}"
                if curl -s -f "http://$load_balancer/health" >/dev/null 2>&1; then
                    success "âœ… AplicaÃ§Ã£o FUNCIONANDO!"
                else
                    warn "âš ï¸  AplicaÃ§Ã£o pode ainda estar inicializando"
                fi
            else
                warn "LoadBalancer ainda nÃ£o disponÃ­vel"
            fi
        else
            warn "Namespace '$APP_NAMESPACE' nÃ£o encontrado. AplicaÃ§Ã£o nÃ£o deployada?"
        fi
        
    else
        echo "âŒ Conectividade kubectl: FALHOU"
        warn "Configure kubectl com: $0 (opÃ§Ã£o 5)"
    fi
    
    echo
    echo -e "${BLUE}=== ğŸ’° CUSTO ESTIMADO ===${NC}"
    echo "ğŸ“Š Recursos ativos que geram custo:"
    echo "   â€¢ EKS Control Plane: ~$0.10/hora (~$73/mÃªs)"
    echo "   â€¢ Node Group (t3.small): ~$0.0208/hora (~$15/mÃªs)"
    echo "   â€¢ EBS Storage: ~$0.10/GB/mÃªs"
    echo "   â€¢ Data Transfer: variÃ¡vel"
    echo
    warn "ğŸ’¡ Para evitar custos, execute limpeza completa quando nÃ£o precisar do ambiente"
    
    success "VerificaÃ§Ã£o de status concluÃ­da!"
}

# Menu principal
main() {
    echo -e "${BLUE}"
    echo "=================================================="
    echo "ğŸ¯ FIAP SOAT - EKS Deploy Script v2.0"
    echo "ğŸ« AWS Academy Optimized"
    echo "ğŸ’¡ Autor: rs94458"
    echo "ğŸ”§ Recursos: Deploy robusto + Limpeza completa"
    echo "=================================================="
    echo -e "${NC}"
    
    check_dependencies
    check_aws_credentials
    
    echo
    echo
    echo "Escolha uma opcao:"
    echo
    echo -e "${CYAN}ğŸ—ï¸  INFRAESTRUTURA:${NC}"
    echo "1) ğŸš€ Deploy completo (infraestrutura + aplicacao)"
    echo "2) ğŸ—ï¸  Apenas infraestrutura EKS"
    echo "3) âš™ï¸  Configurar kubectl"
    echo
    echo -e "${CYAN}ğŸ“¦ APLICACAO NESTJS:${NC}"
    echo "4) ğŸ“¦ Deploy aplicacao NestJS"
    echo "5) ğŸ§¹ Limpar aplicacao"
    echo "6) ğŸ“Š Status aplicacao"
    echo
    echo -e "${CYAN}ğŸ” MONITORAMENTO:${NC}"
    echo "7) ğŸ“Š Status completo (infra + app)"
    echo "8) ğŸ” Verificar recursos AWS"
    echo
    echo -e "${CYAN}ğŸ§¹ LIMPEZA:${NC}"
    echo "9) ğŸ§¹ Limpeza completa (DESTROY ALL)"
    echo "10) ğŸ› ï¸ Limpar recursos orfaos"
    echo "11) ğŸ”§ Limpar state Terraform"
    echo
    echo "0) ğŸ‘‹ Sair"
    echo "0) ğŸ‘‹ Sair"
    echo
    
    read -p "OpÃ§Ã£o: " option
    
    case $option in
        1)
            deploy_infrastructure
            configure_kubectl
            deploy_application
            check_status
            ;;
        2)
            deploy_infrastructure
            configure_kubectl
            ;;
        3)
            configure_kubectl
            ;;
        4)
            configure_kubectl
            deploy_application
            ;;
        5)
            cleanup_application
            ;;
        6)
            check_application_status
            ;;
        7)
            configure_kubectl
            check_status
            ;;
        8)
            check_aws_resources
            ;;
        9)
            cleanup_resources
            ;;
        10)
            cleanup_orphaned_resources
            ;;
        11)
            clean_terraform_state
            ;;
        0)
            success "ğŸ‘‹ Tchau!"
            exit 0
            ;;
        *)
            error "OpÃ§Ã£o invÃ¡lida"
            ;;
    esac
}

# Executar script
main
